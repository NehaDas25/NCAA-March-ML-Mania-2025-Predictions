{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91497,"databundleVersionId":11320667,"sourceType":"competition"}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"try:\n    import cudf\n    import cuml\nexcept ImportError:\n    import os\n    print(\"cuML not found. Installing it now...\")\n    os.system(\"pip install cudf cuml --extra-index-url=https://pypi.nvidia.com\")\n    import cudf\n    import cuml","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport time\nimport torch\nimport itertools\nimport warnings\nimport optuna\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\nimport cupy as cp\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom scipy.stats.mstats import winsorize\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import GridSearchCV\nfrom cuml.linear_model import LogisticRegression as cuLogisticRegression\nfrom cuml.ensemble import RandomForestClassifier as cuRF\nfrom cuml.svm import SVC as cuSVM\nfrom cuml.neighbors import KNeighborsClassifier as cuKNN\nfrom xgboost import XGBClassifier\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.utils._testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ignore all warnings!\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"XGBOOST_VERBOSITY\"] = \"0\"\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"GPU Available:\", torch.cuda.is_available())\nprint(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================\n# Data Loading\n# ==============================\n\n# Set the path for Kaggle dataset (adjust based on your Kaggle Notebook setup)\ndata_path = \"../input/march-machine-learning-mania-2025/\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def safe_read_csv(filepath, usecols=None, dtype=None):\n    \"\"\"Helper function to safely read CSVs, returning None if the file is missing.\"\"\"\n    return pd.read_csv(filepath, usecols=usecols, dtype=dtype) if os.path.exists(filepath) else None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_seed(seed):\n    \"\"\"Extracts numeric seed value from tournament seed data.\"\"\"\n    if isinstance(seed, str) and len(seed) > 1:\n        return int(seed[1:3])\n    return np.nan","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def data_visualization(prefix, tournament_data):\n    \"\"\"\n    Generate data visualizations for:\n    - Win percentages\n    - Point differentials\n    - Tournament seed performance\n    \"\"\"\n\n    gender_label = \"Men's\" if prefix == \"M\" else \"Women's\"\n\n    # Correct column names\n    win_pct_col = \"WinPct\"\n    point_diff_col = \"AvgPointDiff\"\n    seed_col = \"WTeamSeed\"\n\n    # Check if required columns exist\n    if win_pct_col not in tournament_data.columns:\n        print(f\"Warning: {win_pct_col} not found in dataset.\")\n        return\n\n    if point_diff_col not in tournament_data.columns:\n        print(f\"Warning: {point_diff_col} not found in dataset.\")\n        return\n\n    if seed_col not in tournament_data.columns:\n        print(f\"Warning: {seed_col} not found in dataset.\")\n        return\n\n    # Win Percentage Distribution\n    plt.figure(figsize=(10, 6))\n    sns.histplot(tournament_data[win_pct_col], bins=20, kde=True)\n    plt.title(f\"{gender_label} NCAA - Win Percentage Distribution\")\n    plt.xlabel(\"Win Percentage\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n\n    # Point Differential Trends\n    plt.figure(figsize=(10, 6))\n    sns.histplot(tournament_data[point_diff_col], bins=20, kde=True)\n    plt.title(f\"{gender_label} NCAA - Average Point Differential\")\n    plt.xlabel(\"Point Differential\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n\n    # Tournament Seed Performance vs Win Percentage\n    filtered_data = tournament_data.dropna(subset=[seed_col, win_pct_col])\n    if not filtered_data.empty:\n        plt.figure(figsize=(10, 6))\n        sns.boxplot(x=filtered_data[seed_col].astype(int), y=filtered_data[win_pct_col])\n        plt.title(f\"{gender_label} NCAA - Seed Strength vs. Win Percentage\")\n        plt.xlabel(\"Tournament Seed\")\n        plt.ylabel(\"Win Percentage\")\n        plt.show()\n    else:\n        print(f\"Warning: No data available for {seed_col} and {win_pct_col}.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_correlation_heatmap(df, title):\n    \"\"\"\n    Plots a correlation heatmap for the given dataset.\n\n    Parameters:\n    - df: Processed DataFrame with numerical features.\n    - title: Title for the heatmap.\n    \"\"\"\n    plt.figure(figsize=(12, 8))\n    numeric_df = df.select_dtypes(include=[np.number])\n    correlation_matrix = numeric_df.corr()\n    sns.heatmap(\n        correlation_matrix, \n        annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5\n    )\n    plt.title(title)\n    plt.show()\n    time.sleep(20) # Pause for heatmap to show.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_ncaa_data(prefix, data_path):\n    \"\"\"\n    Processes NCAA data for both men and women, including:\n    - Regular season results (compact + detailed)\n    - Tournament results (compact + detailed)\n    - Team information, game cities, conference affiliations\n    - Tournament seeds and slots\n    - Massey Ordinals (for men)\n    - Secondary tournaments\n\n    Parameters:\n    - prefix: 'M' for men's data, 'W' for women's data\n    - data_path: Path to dataset folder\n\n    Returns:\n    - final_data: Merged dataset with features\n    \"\"\"\n\n    dtype_map = {\n        \"Season\": \"int16\", \"TeamID\": \"int16\", \"WTeamID\": \"int16\", \n        \"LTeamID\": \"int16\", \"OrdinalRank\": \"float32\"\n    }\n\n    # Load core datasets\n    teams = pd.read_csv(os.path.join(data_path, f\"{prefix}Teams.csv\"))\n    reg_results = pd.read_csv(os.path.join(data_path, f\"{prefix}RegularSeasonCompactResults.csv\"))\n    detailed_results = pd.read_csv(os.path.join(data_path, f\"{prefix}RegularSeasonDetailedResults.csv\"))\n    tourney_results = pd.read_csv(os.path.join(data_path, f\"{prefix}NCAATourneyCompactResults.csv\"))\n    tourney_detailed = pd.read_csv(os.path.join(data_path, f\"{prefix}NCAATourneyDetailedResults.csv\"))\n    tourney_seeds = pd.read_csv(os.path.join(data_path, f\"{prefix}NCAATourneySeeds.csv\"))\n    tourney_slots = pd.read_csv(os.path.join(data_path, f\"{prefix}NCAATourneySlots.csv\"))\n    team_conferences = pd.read_csv(os.path.join(data_path, f\"{prefix}TeamConferences.csv\"))\n    conf_tourney_results = pd.read_csv(os.path.join(data_path, f\"{prefix}ConferenceTourneyGames.csv\"))\n    secondary_tourney_results = pd.read_csv(os.path.join(data_path, f\"{prefix}SecondaryTourneyCompactResults.csv\"))\n    secondary_tourney_teams = pd.read_csv(os.path.join(data_path, f\"{prefix}SecondaryTourneyTeams.csv\"))\n    game_cities = pd.read_csv(os.path.join(data_path, f\"{prefix}GameCities.csv\"))\n\n    # Load common datasets\n    conferences = pd.read_csv(os.path.join(data_path, \"Conferences.csv\"))\n    cities = pd.read_csv(os.path.join(data_path, \"Cities.csv\"))\n\n    # For Men's Data: Include Massey Ordinals\n    if prefix == \"M\":\n        team_rankings = pd.read_csv(os.path.join(data_path, f\"{prefix}MasseyOrdinals.csv\"), \n                                    usecols=[\"Season\", \"TeamID\", \"OrdinalRank\"])\n        latest_rankings = team_rankings.sort_values([\"Season\", \"TeamID\", \"OrdinalRank\"]).drop_duplicates([\"Season\", \"TeamID\"], keep=\"last\")\n\n    # ==============================\n    # Debugging Column Names\n    # ==============================\n    print(\"\\n===== Checking Columns Before Merge =====\")\n    print(\"Tourney Results Columns:\", tourney_results.columns)\n    print(\"Tourney Seeds Columns:\", tourney_seeds.columns)\n    print(\"Tourney Slots Columns:\", tourney_slots.columns)\n\n    # Ensure `TeamID` exists in `tourney_seeds`\n    if \"TeamID\" not in tourney_seeds.columns:\n        print(\"WARNING: 'TeamID' not found in tourney_seeds! Checking possible alternatives...\")\n        print(\"Available Columns in tourney_seeds:\", tourney_seeds.columns)\n\n    # ==============================\n    # Data Merging\n    # ==============================\n\n    # Merge game cities with cities info\n    game_cities = game_cities.merge(cities, on=\"CityID\", how=\"left\")\n\n    # Merge teams with their conferences\n    team_conferences = team_conferences.merge(conferences, on=\"ConfAbbrev\", how=\"left\")\n\n    # Merge tournament seeds using `WTeamID` and `LTeamID`\n    tourney_results = tourney_results.merge(\n        tourney_seeds.rename(columns={\"TeamID\": \"WTeamID\"}), \n        on=[\"Season\", \"WTeamID\"], \n        how=\"left\"\n    ).rename(columns={\"Seed\": \"WTeamSeed\"})\n\n    tourney_results = tourney_results.merge(\n        tourney_seeds.rename(columns={\"TeamID\": \"LTeamID\"}), \n        on=[\"Season\", \"LTeamID\"], \n        how=\"left\"\n    ).rename(columns={\"Seed\": \"LTeamSeed\"})\n\n    # Merge tournament slots\n    tourney_results = tourney_results.merge(tourney_slots, on=\"Season\", how=\"left\")\n\n    # ==============================\n    # Fix the 'WScore' Issue by Renaming Before Merging\n    # ==============================\n    print(\"Columns in reg_results before merging detailed_results:\", reg_results.columns)\n    print(\"Columns in detailed_results:\", detailed_results.columns)\n\n    # Rename detailed_results columns to prevent overwriting\n    detailed_results = detailed_results.rename(columns={\n        \"WScore\": \"DWScore\",  # Detailed WScore\n        \"LScore\": \"DLScore\"   # Detailed LScore\n    })\n\n    # Merge detailed stats into regular season results\n    reg_results = reg_results.merge(\n        detailed_results[[\"Season\", \"DayNum\", \"WTeamID\", \"LTeamID\", \"DWScore\", \"DLScore\"]],\n        on=[\"Season\", \"DayNum\", \"WTeamID\", \"LTeamID\"],\n        how=\"left\"\n    )\n\n    # Ensure `WScore` and `LScore` exist after merging\n    reg_results[\"WScore\"] = reg_results[\"DWScore\"].fillna(reg_results[\"WScore\"])\n    reg_results[\"LScore\"] = reg_results[\"DLScore\"].fillna(reg_results[\"LScore\"])\n    reg_results.drop(columns=[\"DWScore\", \"DLScore\"], inplace=True)\n\n    # Merge Massey Ordinals (Men's Only)\n    if prefix == \"M\":\n        reg_results = reg_results.merge(\n            latest_rankings.rename(columns={\"TeamID\": \"WTeamID\", \"OrdinalRank\": \"WTeamRank\"}), \n            on=[\"Season\", \"WTeamID\"], \n            how=\"left\"\n        )\n\n        reg_results = reg_results.merge(\n            latest_rankings.rename(columns={\"TeamID\": \"LTeamID\", \"OrdinalRank\": \"LTeamRank\"}), \n            on=[\"Season\", \"LTeamID\"], \n            how=\"left\"\n        )\n\n    # ==============================\n    # Merge Secondary Tournament Data\n    # ==============================\n    print(\"\\n===== Checking Columns Before Merge (Secondary Tournament) =====\")\n    print(\"Secondary Tourney Results Columns:\", secondary_tourney_results.columns)\n    print(\"Secondary Tourney Teams Columns:\", secondary_tourney_teams.columns)\n\n    secondary_tourney_results = secondary_tourney_results.merge(\n        secondary_tourney_teams.rename(columns={\"TeamID\": \"WTeamID\"}), \n        on=[\"Season\", \"WTeamID\"], \n        how=\"left\"\n    )\n\n    secondary_tourney_results = secondary_tourney_results.merge(\n        secondary_tourney_teams.rename(columns={\"TeamID\": \"LTeamID\"}), \n        on=[\"Season\", \"LTeamID\"], \n        how=\"left\"\n    )\n\n    # ==============================\n    # Feature Engineering\n    # ==============================\n\n    # Convert seeds to numeric values\n    tourney_results[\"WTeamSeed\"] = tourney_results[\"WTeamSeed\"].astype(str).apply(lambda x: extract_seed(x) if pd.notna(x) else None)\n    tourney_results[\"LTeamSeed\"] = tourney_results[\"LTeamSeed\"].astype(str).apply(lambda x: extract_seed(x) if pd.notna(x) else None)\n\n    # Calculate Win Percentage\n    win_counts = reg_results.groupby([\"Season\", \"WTeamID\"]).size().reset_index(name=\"Wins\")\n    game_counts = pd.concat([\n        reg_results[[\"Season\", \"WTeamID\"]],\n        reg_results[[\"Season\", \"LTeamID\"]].rename(columns={\"LTeamID\": \"WTeamID\"})\n    ])\n    game_counts = game_counts.groupby([\"Season\", \"WTeamID\"]).size().reset_index(name=\"TotalGames\")\n    win_percentages = win_counts.merge(game_counts, on=[\"Season\", \"WTeamID\"])\n    win_percentages[\"WinPct\"] = win_percentages[\"Wins\"] / win_percentages[\"TotalGames\"]\n\n    # Point Differential per Team per Season\n    reg_results[\"PointDiff\"] = reg_results[\"WScore\"] - reg_results[\"LScore\"]\n    point_diff = reg_results.groupby([\"Season\", \"WTeamID\"])[\"PointDiff\"].mean().reset_index().rename(columns={\"PointDiff\": \"AvgPointDiff\"})\n\n    # Merge Features into Tournament Data\n    final_data = tourney_results.merge(win_percentages, on=[\"Season\", \"WTeamID\"], how=\"left\")\n    final_data = final_data.merge(point_diff, on=[\"Season\", \"WTeamID\"], how=\"left\")\n\n    return final_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Process Men's Data\nmen_data = process_ncaa_data('M', data_path)\nmen_data.to_csv(\"processed_men_data.csv\", index=False)\n\n# Process Women's Data\nwomen_data = process_ncaa_data('W', data_path)\nwomen_data.to_csv(\"processed_women_data.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Processed Men's data\")\nprint(men_data.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize Men's data\ndata_visualization('M', men_data)\n\n\nprint(\"Men's NCAA Data Correlation Heatmap\")\nplot_correlation_heatmap(men_data, \"Men's NCAA Correlation Heatmap\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Processed Women's data\")\nprint(women_data.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize women's data\ndata_visualization('W', women_data)\n\nprint(\"Women's NCAA Data Correlation Heatmap\")\nplot_correlation_heatmap(women_data, \"Women's NCAA Correlation Heatmap\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_ncaa_data(df, drop_cols=None):\n    \"\"\"\n    Preprocess NCAA basketball data for machine learning.\n\n    Parameters:\n    - df: DataFrame with NCAA game results\n    - drop_cols: Columns to remove before training\n    \n    Returns:\n    - X_train, X_test, y_train, y_test: Train/Test feature & target datasets\n    \"\"\"\n\n    if drop_cols is None:\n        drop_cols = [\"Season\", \"DayNum\", \"WTeamID\", \"LTeamID\", \"WLoc\", \"NumOT\"]\n\n    df = df.copy().drop(columns=drop_cols).fillna(0)\n    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n    if categorical_cols:\n        encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n        encoded_features = pd.DataFrame(encoder.fit_transform(df[categorical_cols]))\n        encoded_features.columns = encoder.get_feature_names_out(categorical_cols)\n        df = df.drop(columns=categorical_cols).reset_index(drop=True)\n        df = pd.concat([df, encoded_features], axis=1)\n\n    # Winsorization:\"Cap Extreme Values\"\n    for col in df.columns:\n            df[col] = winsorize(df[col], limits=[0.01, 0.01])\n    \n    win_df = df.drop(columns=[\"LScore\"]).rename(columns={\"WScore\": \"Score\"})\n    win_df[\"Win\"] = 1  # Label for wins\n\n    lose_df = df.drop(columns=[\"WScore\"]).rename(columns={\"LScore\": \"Score\"})\n    lose_df[\"Win\"] = 0  # Label for losses\n\n    final_df = pd.concat([win_df, lose_df])\n\n    X = final_df.drop(columns=[\"Score\", \"Win\"])\n    y = final_df[\"Win\"].astype(int)\n\n    X_new = SelectKBest(f_classif, k=15).fit_transform(X, y)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_new, y, test_size=0.2, random_state=42, stratify=y\n    )\n\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    selected_feature_names = X.columns[SelectKBest(f_classif, k=15).fit(X, y).get_support()]\n    X_train_df = pd.DataFrame(X_train_scaled, columns=selected_feature_names)\n    X_test_df = pd.DataFrame(X_test_scaled, columns=selected_feature_names)\n\n    return X_train_df, X_test_df, y_train, y_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocess men's data\nX_train_men, X_test_men, y_train_men, y_test_men = preprocess_ncaa_data(men_data)\nprint(f\"Y-Train Men Distribution:\\n{y_train_men.value_counts()}\")\nprint(f\"X-Train Men Head:\\n{X_train_men.head()}\")\n\n# Preprocess women's data\nX_train_women, X_test_women, y_train_women, y_test_women = preprocess_ncaa_data(women_data)\nprint(f\"Y-Train Women Distribution:\\n{y_train_women.value_counts()}\")\nprint(f\"X-Train Women Head:\\n{X_train_women.head()}\")\n\nprint(\"Men's and Women's Data Preprocessed for Training!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Support to map tensor to cuda.\nX_train_men = cudf.DataFrame(X_train_men)\nX_test_men = cudf.DataFrame(X_test_men)\ny_train_men = cudf.Series(y_train_men)\ny_test_men = cudf.Series(y_test_men)\n\nX_train_women = cudf.DataFrame(X_train_women)\nX_test_women = cudf.DataFrame(X_test_women)\ny_train_women = cudf.Series(y_train_women)\ny_test_women = cudf.Series(y_test_women)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# MLP ANN\nclass MLPNet(nn.Module):\n    def __init__(self, input_size):\n        super(MLPNet, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(input_size, 256),\n            nn.BatchNorm1d(256),\n            nn.SiLU(),\n            nn.Dropout(0.3),\n\n            nn.Linear(256, 128),\n            nn.BatchNorm1d(128),\n            nn.SiLU(),\n            nn.Dropout(0.3),\n\n            nn.Linear(128, 2),\n            nn.Softmax(dim=1)\n        )  \n\n    def forward(self, x):\n        return self.model(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================\n# Hyperparameter Optimization\n# ==============================\ndef optimize_model(model_class, param_space, X_train, y_train, X_test, y_test, n_trials=20):\n    \"\"\"Optimize hyperparameters using Optuna.\"\"\"\n    # Convert cudf DataFrame to NumPy\n    X_train_np = X_train.to_numpy() if isinstance(X_train, cudf.DataFrame) else X_train\n    y_train_np = y_train.to_numpy() if isinstance(y_train, cudf.Series) else y_train\n    X_test_np = X_test.to_numpy() if isinstance(X_test, cudf.DataFrame) else X_test\n    y_test_np = y_test.to_numpy() if isinstance(y_test, cudf.Series) else y_test\n    def objective(trial):\n        params = {k: v(trial) for k, v in param_space.items()}\n        model = model_class(**params)\n        \n        # Apply cross-validation\n        scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"accuracy\")\n\n        return np.mean(scores)\n\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=n_trials)\n    return study.best_params","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tune_models(X_train, y_train, X_test, y_test):\n    \"\"\"\n    Optimizes hyperparameters for all models using Optuna and GridSearchCV.\n    \"\"\"\n    best_params = {}\n\n    # Convert cudf DataFrame to NumPy for compatibility\n    X_train_np = X_train.to_numpy() if isinstance(X_train, cudf.DataFrame) else X_train\n    y_train_np = y_train.to_numpy() if isinstance(y_train, cudf.Series) else y_train\n    X_test_np = X_test.to_numpy() if isinstance(X_test, cudf.DataFrame) else X_test\n    y_test_np = y_test.to_numpy() if isinstance(y_test, cudf.Series) else y_test\n\n    # Optimizing XGBoost\n    xgb_space = {\n        \"n_estimators\": lambda trial: trial.suggest_int(\"n_estimators\", 100, 1000, step=100),\n        \"max_depth\": lambda trial: trial.suggest_int(\"max_depth\", 3, 10),\n        \"learning_rate\": lambda trial: trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n        \"subsample\": lambda trial: trial.suggest_float(\"subsample\", 0.5, 0.9),\n        \"colsample_bytree\": lambda trial: trial.suggest_float(\"colsample_bytree\", 0.5, 0.9),\n        \"gamma\": lambda trial: trial.suggest_float(\"gamma\", 0, 5),\n        \"min_child_weight\": lambda trial: trial.suggest_int(\"min_child_weight\", 1, 6),\n        \"reg_lambda\": lambda trial: trial.suggest_float(\"reg_lambda\", 0.1, 5.0),\n        \"reg_alpha\": lambda trial: trial.suggest_float(\"reg_alpha\", 0.0, 5.0),\n        \"tree_method\": lambda trial: \"gpu_hist\",\n        \"device\": lambda trial: \"cuda\"\n    }\n    best_params[\"XGBoost\"] = optimize_model(XGBClassifier, xgb_space, X_train_np, y_train_np, X_test_np, y_test_np, n_trials=30)\n\n    # Optimizing Random Forest\n    rf_space = {\n        \"n_estimators\": lambda trial: trial.suggest_int(\"n_estimators\", 100, 1000, step=100),\n        \"max_depth\": lambda trial: trial.suggest_int(\"max_depth\", 5, 20),\n        \"min_samples_split\": lambda trial: trial.suggest_int(\"min_samples_split\", 2, 10),\n        \"min_samples_leaf\": lambda trial: trial.suggest_int(\"min_samples_leaf\", 1, 5),\n        \"max_features\": lambda trial: trial.suggest_float(\"max_features\", 0.5, 1.0),\n    }\n    best_params[\"Random Forest\"] = optimize_model(cuRF, rf_space, X_train_np, y_train_np, X_test_np, y_test_np, n_trials=20)\n\n    # Optimizing SVM\n    svm_space = {\n        \"C\": lambda trial: trial.suggest_float(\"C\", 0.1, 20, log=True),\n        \"kernel\": lambda trial: trial.suggest_categorical(\"kernel\", [\"linear\", \"rbf\"]),\n    }\n    best_params[\"SVM\"] = optimize_model(cuSVM, svm_space, X_train_np, y_train_np, X_test_np, y_test_np, n_trials=20)\n\n    # Optimizing k-NN\n    knn_params = {\"n_neighbors\": [3, 5, 7, 9, 11, 15]}\n    knn_model = cuKNN()\n    grid_knn = GridSearchCV(knn_model, knn_params, cv=5, scoring=\"accuracy\")\n    grid_knn.fit(X_train_np, y_train_np)\n    best_params[\"k-NN\"] = grid_knn.best_params_['n_neighbors']\n\n    return best_params","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_params = tune_models(X_train_men, y_train_men, X_test_men, y_test_men)\nbest_rf_params, best_xgb_params, best_svm_params, best_knn_params = best_params[\"Random Forest\"], best_params[\"XGBoost\"], best_params[\"SVM\"], best_params[\"k-NN\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================\n# Define and Train Models\n# ==============================\ndef get_models(input_size, best_rf_params, best_xgb_params, best_svm_params, best_knn_params):\n    \"\"\"Returns a dictionary of optimized models.\"\"\"\n    return {\n        \"Logistic Regression\": cuLogisticRegression(),\n        \"Random Forest\": cuRF(**best_rf_params),\n        \"XGBoost\": XGBClassifier(**best_xgb_params),\n        \"SVM\": cuSVM(**best_svm_params),\n        \"k-NN\": cuKNN(n_neighbors=best_knn_params),\n        \"Neural Network\": nn.DataParallel(MLPNet(input_size).to(device))\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"models = get_models(X_train_men.shape[1], best_rf_params, best_xgb_params, best_svm_params, best_knn_params)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_models(train_dict, test_dict, models):\n    results = []\n    X_train, y_train = train_dict[\"X_train\"], train_dict[\"y_train\"]\n    X_test, y_test = test_dict[\"X_test\"], test_dict[\"y_test\"]\n\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train.to_numpy())  # Ensure NumPy conversion\n    X_test_scaled = scaler.transform(X_test.to_numpy())\n\n    # Convert target labels to NumPy explicitly\n    y_train_np = y_train.to_numpy() if isinstance(y_train, cudf.Series) else y_train\n    y_test_np = y_test.to_numpy() if isinstance(y_test, cudf.Series) else y_test\n\n    for model_name, model in models.items():\n        print(f\"Training {model_name}...\")\n\n        if model_name == \"Neural Network\":\n            X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n            y_train_torch = torch.tensor(pd.get_dummies(y_train_np).values, dtype=torch.float32).to(device)  # One-hot encoding\n            X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n\n            train_dataset = TensorDataset(X_train_torch, y_train_torch)\n            train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n\n            input_size = X_train.shape[1]\n            mlp_model = MLPNet(input_size).to(device)\n            criterion = nn.CrossEntropyLoss()\n            optimizer = optim.Adam(mlp_model.parameters(), lr=0.0005, weight_decay=1e-5)\n            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)\n\n            num_epochs = 200\n            for epoch in range(num_epochs):\n                mlp_model.train()\n                total_loss = 0\n            \n                for batch_X, batch_y in train_loader:\n                    optimizer.zero_grad()\n                    outputs = mlp_model(batch_X)\n                    loss = criterion(outputs, batch_y)\n                    loss.backward()\n                    optimizer.step()\n                    total_loss += loss.item() * batch_X.size(0)\n            \n                total_loss /= len(train_loader.dataset)\n            \n                scheduler.step()  # No need to pass total_loss\n            \n                if epoch % 10 == 0:\n                    print(f\"Epoch [{epoch}/{num_epochs}], Loss: {total_loss:.4f}\")\n\n\n            with torch.no_grad():\n                y_pred_prob = mlp_model(X_test_torch)\n                y_pred = torch.argmax(y_pred_prob, dim=1).cpu().numpy()\n\n        else:\n            model.fit(X_train_scaled, y_train_np)  # Ensure labels are NumPy\n            y_pred = model.predict(X_test_scaled)\n\n        # Convert predictions explicitly to NumPy\n        y_pred_np = y_pred.to_numpy() if isinstance(y_pred, cudf.Series) else np.array(y_pred)\n\n        results.append([\n            model_name,\n            accuracy_score(y_test_np, y_pred_np),\n            precision_score(y_test_np, y_pred_np, average=\"binary\"),\n            recall_score(y_test_np, y_pred_np, average=\"binary\"),\n            f1_score(y_test_np, y_pred_np, average=\"binary\")\n        ])\n\n    return pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define train and test dictionaries\ntrain_dict_men = {\"X_train\": X_train_men, \"y_train\": y_train_men}\ntest_dict_men = {\"X_test\": X_test_men, \"y_test\": y_test_men}\n\ntrain_dict_women = {\"X_train\": X_train_women, \"y_train\": y_train_women}\ntest_dict_women = {\"X_test\": X_test_women, \"y_test\": y_test_women}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get results for men's models\nmen_results_df = evaluate_models(train_dict_men, test_dict_men, models)\nprint(f\"Men's Results: {men_results_df}\")\n\n# Get results for women's models\nwomen_results_df = evaluate_models(train_dict_women, test_dict_women, models)\nprint(f\"Women's Results: {women_results_df}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_model_performance(results_df, title):\n    \"\"\"\n    Plots the performance of models based on Accuracy, Precision, Recall, and F1 Score.\n\n    Parameters:\n    - results_df: DataFrame containing model performance metrics.\n    - title: Title of the plot.\n    \"\"\"\n    num_models = len(results_df)\n    fig_width = max(10, num_models * 2)\n    fig_height = 6\n    plt.figure(figsize=(fig_width, fig_height))\n    ax = sns.barplot(\n        data=results_df.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Score\"),\n        x=\"Model\", y=\"Score\", hue=\"Metric\"\n    )\n    plt.title(title, fontsize=14)\n    plt.ylabel(\"Score\", fontsize=12)\n    plt.xlabel(\"Model\", fontsize=12)\n    rotation_angle = 0 if num_models <= 4 else 45\n    plt.xticks(rotation=rotation_angle, ha=\"right\", fontsize=10)\n    plt.legend(title=\"Metrics\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure results DataFrame exists before plotting\nif 'men_results_df' in globals() and not men_results_df.empty:\n    plot_model_performance(men_results_df, \"Men's NCAA Model Performance\")\n\nif 'women_results_df' in globals() and not women_results_df.empty:\n    plot_model_performance(women_results_df, \"Women's NCAA Model Performance\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load trained models\nbest_models = {\n    \"Logistic Regression\": cuLogisticRegression(),\n    \"Random Forest\": cuRF(**best_rf_params),\n    \"XGBoost\": XGBClassifier(**best_xgb_params),\n    \"SVM\": cuSVM(**best_svm_params),\n    \"k-NN\": cuKNN(n_neighbors=best_knn_params),\n    \"Neural Network\": nn.DataParallel(MLPNet(X_train_men.shape[1]).to(device)),\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_team_data(gender):\n    \"\"\"Loads preprocessed team statistics for men ('M') or women ('W').\"\"\"\n    file_path = f\"processed_{'men' if gender == 'M' else 'women'}_data.csv\"\n    team_stats = pd.read_csv(file_path)\n    teams = team_stats[\"WTeamID\"].unique()\n    return team_stats, teams","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_matchups(teams):\n    \"\"\"Generates all possible matchups for the tournament.\"\"\"\n    matchups = list(itertools.combinations(teams, 2))\n    return pd.DataFrame(matchups, columns=[\"Team_A\", \"Team_B\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_matchup_features(team_a, team_b, stats_df):\n    \"\"\"Computes feature differences for a given matchup.\"\"\"\n    stats_a = stats_df[stats_df[\"WTeamID\"] == team_a].drop(columns=[\"WTeamID\"]).values\n    stats_b = stats_df[stats_df[\"WTeamID\"] == team_b].drop(columns=[\"WTeamID\"]).values\n\n    if len(stats_a) == 0 or len(stats_b) == 0:\n        return None  # Skip matchups with missing data\n\n    return (stats_a - stats_b).flatten()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_features(matchups, team_stats):\n    \"\"\"Applies feature engineering and standardization.\"\"\"\n    matchup_features = []\n    valid_matchups = []\n\n    for team_a, team_b in tqdm(matchups.values, desc=\"Generating Features\"):\n        features = create_matchup_features(team_a, team_b, team_stats)\n        if features is not None:\n            matchup_features.append(features)\n            valid_matchups.append((team_a, team_b))\n\n    matchup_features_df = pd.DataFrame(matchup_features)\n    valid_matchups_df = pd.DataFrame(valid_matchups, columns=[\"Team_A\", \"Team_B\"])\n\n    # Standardize Features\n    scaler = StandardScaler()\n    matchup_features_scaled = scaler.fit_transform(matchup_features_df)\n\n    # Convert to PyTorch Tensor\n    matchup_features_tensor = torch.tensor(matchup_features_scaled, dtype=torch.float32).cuda()\n\n    return valid_matchups_df, matchup_features_scaled, matchup_features_tensor","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_outcomes(models, features, tensor_features):\n    \"\"\"Predicts outcomes using all trained models.\"\"\"\n    predictions = {}\n    \n    for name, model in models.items():\n        if name == \"Neural Network\":\n            model.eval()\n            with torch.no_grad():\n                outputs = model(tensor_features)\n                preds = (outputs > 0.5).cpu().numpy().astype(int).flatten()\n        else:\n            preds = model.predict(features)\n\n        predictions[name] = preds\n    \n    return predictions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_predictions(gender):\n    \"\"\"Runs the full prediction pipeline for men ('M') or women ('W').\"\"\"\n    print(f\"\\n Processing {gender} tournament predictions...\\n\")\n\n    # Load team statistics and matchups\n    team_stats, teams = load_team_data(gender)\n    matchups = generate_matchups(teams)\n\n    # Prepare features\n    valid_matchups_df, matchup_features_scaled, matchup_features_tensor = prepare_features(matchups, team_stats)\n\n    # Get model predictions\n    predictions = predict_outcomes(best_models, matchup_features_scaled, matchup_features_tensor)\n\n    # Ensemble Strategy (Majority Voting)\n    ensemble_preds = np.round(\n        (predictions[\"Random Forest\"] + predictions[\"XGBoost\"] + predictions[\"SVM\"]) / 3\n    )\n\n    # Format Predictions for Kaggle Submission\n    submission_df = valid_matchups_df.copy()\n    submission_df[\"Pred\"] = ensemble_preds\n    submission_df[\"ID\"] = submission_df[\"Team_A\"].astype(str) + \"_\" + submission_df[\"Team_B\"].astype(str)\n    submission_df = submission_df[[\"ID\", \"Pred\"]]\n\n    # Save to CSV\n    file_name = f\"submission_{'men' if gender == 'M' else 'women'}.csv\"\n    submission_df.to_csv(file_name, index=False)\n    print(f\"Submission file saved as {file_name}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run predictions for both tournaments\ngenerate_predictions(\"M\")  # Men's tournament\ngenerate_predictions(\"W\")  # Women's tournament","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}